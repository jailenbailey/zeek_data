{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "#### Can't go anywhere without those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation/analysis libraries\n",
    "import dtale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plotting/visual ibraries\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "color_pal = sns.color_palette()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.display import clear_output\n",
    "# Models/performance/metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group and Resample\n",
    "#### Simple logic to create the main feature, which will be our y, or our target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cleaning/grouping/resampling function to create generically useable data\"\"\"\n",
    "def groupby_hour(base_df):\n",
    "  base_df['ts'] = pd.to_datetime(base_df['ts'])\n",
    "  # Parse the time to remove all unwanted values\n",
    "  base_df['ts'] = base_df['ts'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "  # Create a new frame to group the unique values by size\n",
    "  df = base_df.groupby(['ts']).size()\n",
    "  # Reassign the new frame to create the new column\n",
    "  df = base_df.join(df.to_frame(), on='ts')\n",
    "  # Drop all duplicate entries, leaving only the unique entries\n",
    "  df = df.drop_duplicates(subset=['ts'])\n",
    "  # New df to not have to list all columns to drop\n",
    "  zeek_df = df[['ts', 0]]\n",
    "  # Rename the column\n",
    "  zeek_df = zeek_df.rename(columns={0 : 'log_counts'})\n",
    "  # Set the index to the ts column\n",
    "  zeek_df = zeek_df.set_index('ts')\n",
    "  # Set index to a type of datetime\n",
    "  zeek_df.index = pd.to_datetime(zeek_df.index)\n",
    "  # Finally, group by hour\n",
    "  zeek_df = zeek_df.resample('H').sum()\n",
    "  # Return newly formatted df\n",
    "  return zeek_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "#### Using Pandas datetime functions (thank you pandas) for new features to feed to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to create specific features for the model\"\"\"\n",
    "def create_features(df):\n",
    "  # Call the groupby_hour function\n",
    "  df = groupby_hour(df)\n",
    "  # Make month column\n",
    "  df['month'] = df.index.month\n",
    "  # Make day column\n",
    "  df['day'] = df.index.day\n",
    "  # Make hour column\n",
    "  df['hour'] = df.index.hour\n",
    "  # Make day of year column\n",
    "  df['day_of_year'] = df.index.day_of_year\n",
    "  # Return df\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features\n",
    "#### What was the value of our target (x) days in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init function\n",
    "def add_lags(df):\n",
    "  # Call the create features function\n",
    "  df = create_features(df)\n",
    "  # Create a target map\n",
    "  target_map = df['log_counts'].to_dict()\n",
    "  # Create lag feature for 1 day in the past\n",
    "  df['lag1'] = (df.index - pd.Timedelta('1 day')).map(target_map)\n",
    "  # Create lag feature for 7 days in the past\n",
    "  df['lag7'] = (df.index - pd.Timedelta('7 days')).map(target_map)\n",
    "  # Create lag feature for 14 days in the past\n",
    "  df['lag14'] = (df.index - pd.Timedelta('14 days')).map(target_map)\n",
    "  # Return finished function\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "#### This notebook will function a bit differently from the 5 minute model, as in everything from here on will be more experimental, to include visualizations for confirmation/comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv\n",
    "df = pd.read_csv('')\n",
    "# Call the function and re-declare the df we already created\n",
    "df = add_lags(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign features to X and the target to y\n",
    "#### Using SKlearns train_test_split's convenience, I am going to rearrange the columns, create X and y values from columns based off the index location, and then create the train/test split variables to feed our model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "df = df[['month', 'day', 'day_of_year', 'hour', 'lag1', 'lag7', 'lag14', 'log_counts']]\n",
    "# Create features columns later\n",
    "FEATURES = ['month', 'day', 'day_of_year', 'hour', 'lag1', 'lag7', 'lag14']\n",
    "# X Value to get everything but the last column in the DF\n",
    "X = df.iloc[:, :-1]\n",
    "# y value to get only the last column in the DF\n",
    "y = df.iloc[:, -1]\n",
    "# Using SKlearns train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "# Create the model\n",
    "reg = xgb.XGBRegressor(base_score=0.5,\n",
    "                       booster='gbtree',    \n",
    "                       n_estimators=100,\n",
    "                       objective='reg:squarederror',\n",
    "                       max_depth=3,\n",
    "                       learning_rate=0.01,\n",
    "                       colsample_bytree=0.7,\n",
    "                       colsample_bylevel=0.4,\n",
    "                       subsample=0.89)\n",
    "# Fit the model with our train/test data  \n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is definitely useful, but not entirely necessary, as it is showing us the most useful features for the model\n",
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=['importance'])\n",
    "# Creating feature importance plot  \n",
    "fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "# Present the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a future date range to predict on\n",
    "future = pd.date_range('2022-12-01 00:00:00', '2022-12-31 23:00:00', freq='1h')\n",
    "# Create a future df with the date range as index\n",
    "future_hourly_df = pd.DataFrame(index=future)\n",
    "# Create a boolean based series set to True prior to concatting with existing data\n",
    "future_hourly_df['isFuture'] = True\n",
    "# New boolean feature set to False for future querying\n",
    "df['isFuture'] = False\n",
    "# Concat the two frames\n",
    "df_and_future = pd.concat([df, future_hourly_df])\n",
    "# Calling the create features function for fresh features\n",
    "df_and_future = create_features(df_and_future)\n",
    "# Calling the add_lags function for fresh features\n",
    "df_and_future = add_lags(df_and_future)\n",
    "# Creating a new df with only True isFuture values by querying\n",
    "future_and_features = df_and_future.query('isFuture').copy()\n",
    "# Making a new column for predictions and predicting for those values\n",
    "future_and_features['pred'] = reg.predict(future_and_features[FEATURES])\n",
    "# Reassigning the predictions column to compare to historical values\n",
    "df_and_future['pred'] = future_and_features['pred']\n",
    "# Confirm it works\n",
    "df_and_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before comparing, lets plot to get a glimpse of how the predictions look\n",
    "future_and_features['pred'].plot(figsize=(10, 5),\n",
    "                               color=color_pal[4],\n",
    "                               ms=1,\n",
    "                               lw=1,\n",
    "                               title='Future Predictions')\n",
    "# Plot the visualization \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# Plot pre-existing data\n",
    "df['log_counts'].plot(ax=ax, label='Pre-Existing Data', title='Prediction with Pre-Existing Data')\n",
    "# Plot predicted data\n",
    "future_and_features['pred'].plot(ax=ax, label='Prediction')\n",
    "# Set the plot legend\n",
    "ax.legend(['Existing Data', 'Prediction Data'])\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning\n",
    "#### This is some simple logic that allows me to hyper-tune the parameters of the model to enhance the results, and prevent overfitting. This isn't a part of the regular logic, as this has already been used to tune this model. However, it is absolutely useful and worth keeping handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X and y values for every hour\"\"\"\n",
    "# Create dataset specific features\n",
    "FEATURES = ['month', 'day', 'hour', 'lag1', 'lag7', 'lag14']\n",
    "# Create a value for the target\n",
    "TARGET = 'log_counts'\n",
    "\n",
    "# X value with the features\n",
    "X = df[FEATURES]\n",
    "# y value with the target\n",
    "y = df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a list of each parameter to test\n",
    "params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "# Create the normal model \n",
    "xgbr = xgb.XGBRegressor(seed = 20)\n",
    "# Create the grid-search and assign the model and params to it\n",
    "clf = GridSearchCV(estimator=xgbr, \n",
    "                   param_grid=params,\n",
    "                   scoring='neg_mean_squared_error', \n",
    "                   verbose=1)\n",
    "# Fit the GridSearchCV  \n",
    "clf.fit(X, y)\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "# Print lowest root mean squared error\n",
    "print(\"Lowest RMSE: \", (-clf.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "677aa2d05f4e9a22a85b022494cb837b2b4dbe7e4c4ae1765326679f067116b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
