{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "#### Can't go anywhere without those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation/analysis libraries\n",
    "import dtale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plotting/visual ibraries\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "color_pal = sns.color_palette()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.display import clear_output\n",
    "# Models/performance/metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group and Resample\n",
    "#### Simple logic to create the main feature, which will be our y, or our target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cleaning/grouping/resampling function to create generically useable data\"\"\"\n",
    "def groupby_time(base_df):\n",
    "  # Convert to DateTime\n",
    "  base_df['ts'] = pd.to_datetime(base_df['ts'])\n",
    "  # Parse the time to remove all unwanted values\n",
    "  base_df['ts'] = base_df['ts'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "  # Create a new frame to group the unique values by size\n",
    "  df = base_df.groupby(['ts']).size()\n",
    "  # Reassign the new frame to create the new column\n",
    "  df = base_df.join(df.to_frame(), on='ts')\n",
    "  # Drop all duplicate entries, leaving only the unique entries\n",
    "  df = df.drop_duplicates(subset=['ts'])\n",
    "  # New df to not have to list all columns to drop\n",
    "  zeek_df = df[['ts', 0]]\n",
    "  # Rename the column\n",
    "  zeek_df = zeek_df.rename(columns={0 : 'log_counts'})\n",
    "  # Set the index to the ts column\n",
    "  zeek_df = zeek_df.set_index('ts')\n",
    "  # Set index to a type of datetime\n",
    "  zeek_df.index = pd.to_datetime(zeek_df.index)\n",
    "  # Finally, group by hour\n",
    "  zeek_df = zeek_df.resample('5min').sum()\n",
    "  # Return newly formatted df\n",
    "  return zeek_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "#### Using Pandas datetime functions (thank you pandas) for new features to feed to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to create specific features for the model\"\"\"\n",
    "def create_features(df):\n",
    "  # Call the grouping function\n",
    "  df = groupby_time(df)\n",
    "  # Make month column\n",
    "  df['month'] = df.index.month\n",
    "  # Make day column\n",
    "  df['day'] = df.index.day\n",
    "  # Make hour column\n",
    "  df['hour'] = df.index.hour\n",
    "  # Make minute column\n",
    "  df['minute'] = df.index.minute\n",
    "  # Make day of year column\n",
    "  df['day_of_year'] = df.index.day_of_year\n",
    "  # Return df\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features\n",
    "#### What was the value of our target (x) days in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating Lag Features as another option to feed information to the model\"\"\"\n",
    "def add_lags(df):\n",
    "  # Call the create features function\n",
    "  df = create_features(df)\n",
    "  # Create a target map\n",
    "  target_map = df['log_counts'].to_dict()\n",
    "  # Create lag feature for 1 day in the past\n",
    "  df['lag1'] = (df.index - pd.Timedelta('1 day')).map(target_map)\n",
    "  # Create lag feature for 7 days in the past\n",
    "  df['lag7'] = (df.index - pd.Timedelta('7 days')).map(target_map)\n",
    "  # Create lag feature for 14 days in the past\n",
    "  df['lag14'] = (df.index - pd.Timedelta('14 days')).map(target_map)\n",
    "  # Return finished function\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign features to X and the target to y\n",
    "#### Using SKlearns train_test_split's convenience, I am going to rearrange the columns, create X and y values from columns based off the index location, and then create the train/test split variables to feed our model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to divide the data by features and percentage, and then train/fit the model\"\"\"\n",
    "def split_train_test_model(df):\n",
    "  # Call the add_lags function\n",
    "  df = add_lags(df)\n",
    "  # Rearrange the columns\n",
    "  df = df[['month', 'day', 'hour', 'minute', 'day_of_year', 'lag1', 'lag7', 'lag14', 'log_counts']]\n",
    "  # X Value to get everything but the last column in the DF\n",
    "  X = df.iloc[:, :-1]\n",
    "  # y value to get only the last column in the DF\n",
    "  y = df.iloc[:, -1]\n",
    "  # Using SKlearns train_test_split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "  # Create model\n",
    "  reg = xgb.XGBRegressor(base_score=0.5,\n",
    "                       booster='gbtree',    \n",
    "                       n_estimators=100,\n",
    "                       objective='reg:squarederror',\n",
    "                       max_depth=3,\n",
    "                       learning_rate=0.01,\n",
    "                       colsample_bytree=0.79,\n",
    "                       colsample_bylevel=0.4,\n",
    "                       subsample=0.89)\n",
    "  # Fit the model \n",
    "  reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=100)\n",
    "  # Return the trained model\n",
    "  return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the data and run the model\"\"\"\n",
    "df = pd.read_csv('')\n",
    "model = train_test_split(df)\n",
    "model.save_model('xbg_reg_zeek_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "677aa2d05f4e9a22a85b022494cb837b2b4dbe7e4c4ae1765326679f067116b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
